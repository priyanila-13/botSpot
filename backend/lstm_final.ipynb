{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "dxhnFWVrjs5V"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "v4c5iSzXP9uk"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"test_notepad_path = 'phase1/annotations/humans_and_advanced_bots/test.txt'\\ntrain_notepad_path = 'phase1/annotations/humans_and_advanced_bots/test.txt'\\njson_directory_path = 'phase1/data/mouse_movements/humans_and_advanced_bots'\\nsession_folders = os.listdir(json_directory_path)\""
            ]
          },
          "execution_count": 151,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''test_notepad_path = 'phase1/annotations/humans_and_advanced_bots/test.txt'\n",
        "train_notepad_path = 'phase1/annotations/humans_and_advanced_bots/test.txt'\n",
        "json_directory_path = 'phase1/data/mouse_movements/humans_and_advanced_bots'\n",
        "session_folders = os.listdir(json_directory_path)'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "9FgGVIOhQAnI"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"# Function to read session IDs and their labels\\ndef read_session_labels(file_path):\\n    session_labels = {}\\n    with open(file_path, 'r') as file:\\n        for line in file:\\n            parts = line.strip().split()\\n            if len(parts) == 2:\\n                session_id, label = parts\\n                session_labels[session_id] = label\\n    return session_labels\\n\\n# Read session IDs and labels from both test and train files\\ntest_session_labels = read_session_labels(test_notepad_path)\\ntrain_session_labels = read_session_labels(train_notepad_path)\\n\\n# Combine test and train labels\\nall_session_labels = {**test_session_labels, **train_session_labels}\""
            ]
          },
          "execution_count": 152,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "'''# Function to read session IDs and their labels\n",
        "def read_session_labels(file_path):\n",
        "    session_labels = {}\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) == 2:\n",
        "                session_id, label = parts\n",
        "                session_labels[session_id] = label\n",
        "    return session_labels\n",
        "\n",
        "# Read session IDs and labels from both test and train files\n",
        "test_session_labels = read_session_labels(test_notepad_path)\n",
        "train_session_labels = read_session_labels(train_notepad_path)\n",
        "\n",
        "# Combine test and train labels\n",
        "all_session_labels = {**test_session_labels, **train_session_labels}'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kgGm87BQSj7",
        "outputId": "afd80611-40ac-4867-91e3-3d2e27895f2a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"import csv\\n\\n# Define input and output file paths\\ninput_file = 'phase2/annotations/humans_and_moderate_and_advanced_bots/humans_and_moderate_and_advanced_bots.txt'  # Path to your text file\\noutput_file = 'phase2/annotations/humans_and_moderate_and_advanced_bots/humans_and_moderate_and_advanced_bots.csv'  # Path for the output CSV file\\n\\n# Read data from the input text file\\nwith open(input_file, 'r') as file:\\n    lines = file.readlines()\\n\\n# Process the data into a list of tuples (session_id, result)\\ndata = [tuple(line.strip().split()) for line in lines]\\n\\n# Write the processed data into the CSV file\\nwith open(output_file, mode='w', newline='') as file:\\n    writer = csv.writer(file)\\n    writer.writerow(['session_id', 'result'])  # Write the header\\n    writer.writerows(data)  # Write the session data\\n\\nprint(f'Data has been converted to {output_file}')\""
            ]
          },
          "execution_count": 153,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''import csv\n",
        "\n",
        "# Define input and output file paths\n",
        "input_file = 'phase2/annotations/humans_and_moderate_and_advanced_bots/humans_and_moderate_and_advanced_bots.txt'  # Path to your text file\n",
        "output_file = 'phase2/annotations/humans_and_moderate_and_advanced_bots/humans_and_moderate_and_advanced_bots.csv'  # Path for the output CSV file\n",
        "\n",
        "# Read data from the input text file\n",
        "with open(input_file, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Process the data into a list of tuples (session_id, result)\n",
        "data = [tuple(line.strip().split()) for line in lines]\n",
        "\n",
        "# Write the processed data into the CSV file\n",
        "with open(output_file, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['session_id', 'result'])  # Write the header\n",
        "    writer.writerows(data)  # Write the session data\n",
        "\n",
        "print(f'Data has been converted to {output_file}')'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "oas5t8XBkAIu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   session_id result\n",
            "0  dr09rk5eagjuu87gedvdqmq3gl  human\n",
            "1  gq715ms79515gcq39vf91mli6t  human\n",
            "2  hrbko2t4t14q3pahqltndlolb5  human\n",
            "3  nvmlnfhs5v6hehsd81e9mf75cn  human\n",
            "4  brrlh9tmiodt2ekkjvn7kcsps0  human\n",
            "                   session_id result\n",
            "0  g2gh9qmk9krld14h5uojlg7g10  human\n",
            "1  kaodsjbnqm7umgfvao63d3rihb  human\n",
            "2  1aqgqrcuurlmvvbbpirvsh7e53  human\n",
            "3  igbeqcjnbst8afmoi4sg6tn669  human\n",
            "4  vopb1c4o3o2dpsov8jinbbou5h  human\n"
          ]
        }
      ],
      "source": [
        "# Define the paths to the CSV files\n",
        "test_csv_paths = ['phase1/annotations/humans_and_advanced_bots/test.csv',\n",
        "                 'phase1/annotations/humans_and_moderate_bots/test.csv',\n",
        "                 #'G:\\web_bot_detection_dataset\\phase2\\\\annotations\\humans_and_advanced_bots\\\\humans_and_advanced_bots.csv'\n",
        "                 ]\n",
        "\n",
        "train_csv_paths = ['phase1/annotations/humans_and_advanced_bots/train.csv',\n",
        "                  'phase1/annotations/humans_and_moderate_bots/train.csv',\n",
        "                  #'G:\\web_bot_detection_dataset\\phase2\\\\annotations\\humans_and_moderate_and_advanced_bots\\\\humans_and_moderate_and_advanced_bots.csv'\n",
        "                  ]\n",
        "\n",
        "json_directory_path = ['phase1/data/mouse_movements/humans_and_advanced_bots',\n",
        "                       'phase1/data/mouse_movements/humans_and_moderate_bots',\n",
        "                       #'G:\\web_bot_detection_dataset\\phase2\\data\\mouse_movements\\\\bots',\n",
        "                       #'G:\\web_bot_detection_dataset\\phase2\\data\\mouse_movements\\\\bots'\n",
        "                       ]\n",
        "\n",
        "# Function to load session data from multiple CSV files\n",
        "def load_session_data(csv_paths):\n",
        "    data_frames = []\n",
        "    for path in csv_paths:\n",
        "        df = pd.read_csv(path)\n",
        "        data_frames.append(df[['session_id', 'result']])  # Load only the session_id and result columns\n",
        "    return pd.concat(data_frames, ignore_index=True)  # Combine all dataframes into one\n",
        "\n",
        "# Load train and test datasets\n",
        "train_df = load_session_data(train_csv_paths)\n",
        "test_df = load_session_data(test_csv_paths)\n",
        "\n",
        "# Check the combined dataframes\n",
        "print(train_df.head())\n",
        "print(test_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "PvmjryOwnqUQ"
      },
      "outputs": [],
      "source": [
        "def load_mouse_movement_data(session_id, base_paths):\n",
        "    for base_path in base_paths:\n",
        "        file_path = os.path.join(base_path, session_id, 'mouse_movements.json')\n",
        "        if os.path.isfile(file_path):\n",
        "            try:\n",
        "                with open(file_path, 'r') as f:\n",
        "                    mouse_data = json.load(f)\n",
        "                return mouse_data\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"Error decoding JSON from file: {file_path}\")\n",
        "                continue\n",
        "    print(f\"File not found for session ID {session_id} in any of the provided paths.\")\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "EFX1hywUnwlH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "\n",
        "import re\n",
        "\n",
        "def parse_mouse_movements(total_behaviour):\n",
        "    # Regular expression to extract coordinates\n",
        "    pattern = re.compile(r'\\[m\\((\\d+),(\\d+)\\)\\]')\n",
        "    matches = pattern.findall(total_behaviour)\n",
        "    \n",
        "    # Convert to list of tuples\n",
        "    coordinates = [(int(x), int(y)) for x, y in matches]\n",
        "    \n",
        "    return coordinates\n",
        "\n",
        "def extract_lstm_features(mouse_data):\n",
        "    # Parse the mouse movements\n",
        "    coordinates = parse_mouse_movements(mouse_data.get('total_behaviour', ''))\n",
        "    \n",
        "    if not coordinates:\n",
        "        return np.zeros((20, 2))  # Return a zero-filled array if no movements\n",
        "\n",
        "    x_coords, y_coords = zip(*coordinates) if coordinates else ([], [])\n",
        "\n",
        "    # Normalize or pad the sequences to a fixed length for LSTM\n",
        "    max_length = 20  # Example length\n",
        "    x_coords = list(x_coords[:max_length]) + [0] * (max_length - len(x_coords))\n",
        "    y_coords = list(y_coords[:max_length]) + [0] * (max_length - len(y_coords))\n",
        "\n",
        "    # Combine x and y coordinates into a 2D array\n",
        "    features = np.column_stack((x_coords, y_coords))\n",
        "\n",
        "    return features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "yiPlReiDnyse"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train features shape: (140, 20, 2)\n",
            "Test features shape: (60, 20, 2)\n"
          ]
        }
      ],
      "source": [
        "def extract_features_from_multiple_paths(df, base_paths):\n",
        "    features, labels = [], []\n",
        "    for session_id, label in zip(df['session_id'], df['result']):\n",
        "        mouse_data = load_mouse_movement_data(session_id, base_paths)\n",
        "        if mouse_data:\n",
        "            features.append(extract_lstm_features(mouse_data))\n",
        "            labels.append(1 if label == 'human' else 0)\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "def load_session_data(csv_paths):\n",
        "    data_frames = []\n",
        "    for path in csv_paths:\n",
        "        df = pd.read_csv(path)\n",
        "        data_frames.append(df[['session_id', 'result']])\n",
        "    return pd.concat(data_frames, ignore_index=True)\n",
        "\n",
        "\n",
        "train_df = load_session_data(train_csv_paths)\n",
        "test_df = load_session_data(test_csv_paths)\n",
        "\n",
        "train_features, train_labels = extract_features_from_multiple_paths(train_df, json_directory_path)\n",
        "test_features, test_labels = extract_features_from_multiple_paths(test_df, json_directory_path)\n",
        "\n",
        "print(f\"Train features shape: {train_features.shape}\")\n",
        "print(f\"Test features shape: {test_features.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "result\n",
            "human           70\n",
            "advanced_bot    35\n",
            "moderate_bot    35\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "def prepare_data(csv_paths, base_paths):\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "\n",
        "    for csv_path in csv_paths:\n",
        "        df = load_session_data([csv_path])  # Pass the single path as a list\n",
        "\n",
        "        for _, row in df.iterrows():\n",
        "            session_id = row['session_id']\n",
        "            label = row['result']\n",
        "\n",
        "            mouse_data = load_mouse_movement_data(session_id, base_paths)\n",
        "            if mouse_data:\n",
        "                features = extract_lstm_features(mouse_data)\n",
        "                all_features.append(features)\n",
        "                all_labels.append(1 if label == 'human' else 0)  # 1 for human, 0 for bot\n",
        "\n",
        "    return np.array(all_features), np.array(all_labels)\n",
        "\n",
        "\n",
        "# Use file paths directly, not DataFrames\n",
        "train_features, train_labels = prepare_data(train_csv_paths, json_directory_path)\n",
        "test_features, test_labels = prepare_data(test_csv_paths, json_directory_path)\n",
        "print(train_df['result'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBeChP2wn1XB",
        "outputId": "45af3a92-d95a-4a12-b245-cb076128a119"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - accuracy: 0.6311 - loss: 0.6241 - val_accuracy: 0.9833 - val_loss: 0.4239\n",
            "Epoch 2/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.3867 - val_accuracy: 1.0000 - val_loss: 0.2440\n",
            "Epoch 3/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.2168 - val_accuracy: 1.0000 - val_loss: 0.1015\n",
            "Epoch 4/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0909 - val_accuracy: 1.0000 - val_loss: 0.0399\n",
            "Epoch 5/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0356 - val_accuracy: 1.0000 - val_loss: 0.0164\n",
            "Epoch 6/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0160 - val_accuracy: 1.0000 - val_loss: 0.0071\n",
            "Epoch 7/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0066 - val_accuracy: 1.0000 - val_loss: 0.0036\n",
            "Epoch 8/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 0.0022\n",
            "Epoch 9/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 1.0000 - val_loss: 0.0015\n",
            "Epoch 10/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 1.0000 - val_loss: 0.0011\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x1ebc3481b50>"
            ]
          },
          "execution_count": 159,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "# One-hot encode the labels\n",
        "train_labels = to_categorical(train_labels, num_classes=2)\n",
        "test_labels = to_categorical(test_labels, num_classes=2)\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(20, 2), return_sequences=True))\n",
        "model.add(LSTM(64))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_features, train_labels, epochs=10, batch_size=32, validation_data=(test_features, test_labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxPGeH2PoVzM",
        "outputId": "8f2b6dae-c103-4ebf-d1c9-773c40e7989c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step\n",
            "Accuracy: 1.0\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         bot       1.00      1.00      1.00        30\n",
            "       human       1.00      1.00      1.00        30\n",
            "\n",
            "    accuracy                           1.00        60\n",
            "   macro avg       1.00      1.00      1.00        60\n",
            "weighted avg       1.00      1.00      1.00        60\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Predict the test data\n",
        "test_predictions = model.predict(test_features)\n",
        "test_predictions = np.argmax(test_predictions, axis=1)\n",
        "true_labels = np.argmax(test_labels, axis=1)\n",
        "\n",
        "# Calculate accuracy and print classification report\n",
        "accuracy = accuracy_score(true_labels, test_predictions)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(\"Classification Report:\\n\", classification_report(true_labels, test_predictions, target_names=['bot', 'human']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4dX6cPYoYqq",
        "outputId": "f4094cb5-8cb1-4e06-b39b-b2b472d11173"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "# Save the trained model\n",
        "model.save('lstm_mouse_movement_classifier.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6B5DX2AzhZZ",
        "outputId": "489142ca-d66c-44d8-b1f3-5f5a1c72f27b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step\n",
            "Predicted Label: bot\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "\n",
        "# Sample test session\n",
        "test_session = {\n",
        "    \"session_id\": \"jfmilo33fin84baeh3k6bcnh3v\",\n",
        "    \"total_behaviour\": \"[m(0,4)][m(0,5)][m(0,6)][m(0,7)][m(1,8)][m(1,9)][m(1,10)]\"\n",
        "}\n",
        "\n",
        "# Function to load and extract features for a single session\n",
        "def load_mouse_movement_data_from_sample(test_session):\n",
        "    return test_session['total_behaviour']\n",
        "\n",
        "# Feature extraction remains the same\n",
        "def extract_lstm_features(mouse_movements):\n",
        "    x_coords, y_coords = [], []\n",
        "\n",
        "    # Use regular expressions to find all coordinate entries\n",
        "    matches = re.findall(r'm\\((\\d+),(\\d+)\\)', mouse_movements)\n",
        "\n",
        "    if not matches:\n",
        "        return np.zeros((20, 2))  # Return a zero-filled array if no movements\n",
        "\n",
        "    # Convert matches to integer lists\n",
        "    for match in matches:\n",
        "        x, y = map(int, match)\n",
        "        x_coords.append(x)\n",
        "        y_coords.append(y)\n",
        "\n",
        "    # Normalize or pad the sequences to a fixed length for LSTM\n",
        "    max_length = 20  # Example length\n",
        "    x_coords = x_coords[:max_length] + [0] * (max_length - len(x_coords))\n",
        "    y_coords = y_coords[:max_length] + [0] * (max_length - len(y_coords))\n",
        "\n",
        "    # Combine x and y coordinates into a 2D array\n",
        "    features = np.column_stack((x_coords, y_coords))\n",
        "\n",
        "    return features\n",
        "\n",
        "# Extract features for the test session\n",
        "test_mouse_data = load_mouse_movement_data_from_sample(test_session)\n",
        "test_features = extract_lstm_features(test_mouse_data)\n",
        "\n",
        "# Reshape the features to 3D (1, 20, 2) for a single test sample\n",
        "test_features = np.expand_dims(test_features, axis=0)\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model('lstm_mouse_movement_classifier.h5')\n",
        "\n",
        "# Make predictions\n",
        "test_prediction = model.predict(test_features)\n",
        "predicted_label = np.argmax(test_prediction, axis=1)[0]\n",
        "\n",
        "# Map the label back to human or bot\n",
        "label_mapping = {0: 'bot', 1: 'human'}\n",
        "print(f\"Predicted Label: {label_mapping[predicted_label]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJ-8k82N_QJ3",
        "outputId": "5158d050-f790-46ec-f34a-701f74bb6909"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step\n",
            "Predicted Label: human\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "\n",
        "# Sample test session\n",
        "test_session = {\n",
        "    \"session_id\": \"g2gh9qmk9krld14h5uojlg7g10\",\n",
        "    \"total_behaviour\": \"[m(627,558)][m(626,555)][m(625,552)][m(624,548)][m(622,539)][m(622,535)][m(622,531)][m(622,525)][m(620,518)][m(614,506)][m(607,496)][m(602,488)][m(601,483)][m(601,482)][m(601,481)][m(600,479)][m(600,476)][m(598,475)][m(595,472)][m(592,469)][m(587,467)][m(583,467)][m(578,467)][m(576,466)][m(574,465)][m(572,465)][m(567,464)][m(558,461)][m(551,458)][m(547,455)][m(545,453)][m(542,448)][m(538,441)][m(533,435)]\"\n",
        "}\n",
        "\n",
        "# Function to load and extract features for a single session\n",
        "def load_mouse_movement_data_from_sample(test_session):\n",
        "    return test_session['total_behaviour']\n",
        "\n",
        "# Feature extraction remains the same\n",
        "def extract_lstm_features(mouse_movements):\n",
        "    x_coords, y_coords = [], []\n",
        "\n",
        "    # Use regular expressions to find all coordinate entries\n",
        "    matches = re.findall(r'm\\((\\d+),(\\d+)\\)', mouse_movements)\n",
        "\n",
        "    if not matches:\n",
        "        return np.zeros((20, 2))  # Return a zero-filled array if no movements\n",
        "\n",
        "    # Convert matches to integer lists\n",
        "    for match in matches:\n",
        "        x, y = map(int, match)\n",
        "        x_coords.append(x)\n",
        "        y_coords.append(y)\n",
        "\n",
        "    # Normalize or pad the sequences to a fixed length for LSTM\n",
        "    max_length = 20  # Example length\n",
        "    x_coords = x_coords[:max_length] + [0] * (max_length - len(x_coords))\n",
        "    y_coords = y_coords[:max_length] + [0] * (max_length - len(y_coords))\n",
        "\n",
        "    # Combine x and y coordinates into a 2D array\n",
        "    features = np.column_stack((x_coords, y_coords))\n",
        "\n",
        "    return features\n",
        "\n",
        "# Extract features for the test session\n",
        "test_mouse_data = load_mouse_movement_data_from_sample(test_session)\n",
        "test_features = extract_lstm_features(test_mouse_data)\n",
        "\n",
        "# Reshape the features to 3D (1, 20, 2) for a single test sample\n",
        "test_features = np.expand_dims(test_features, axis=0)\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model('lstm_mouse_movement_classifier.h5')\n",
        "\n",
        "# Make predictions\n",
        "test_prediction = model.predict(test_features)\n",
        "predicted_label = np.argmax(test_prediction, axis=1)[0]\n",
        "\n",
        "# Map the label back to human or bot\n",
        "label_mapping = {0: 'bot', 1: 'human'}\n",
        "print(f\"Predicted Label: {label_mapping[predicted_label]}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
